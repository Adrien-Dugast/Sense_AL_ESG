{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import csv\n",
    "import newspaper\n",
    "import requests\n",
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clarticle:\n",
    "    def __init__(self,title,url,date,author,text):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.date = date\n",
    "        self.author = author\n",
    "        self.text = text\n",
    "\n",
    "\n",
    "# def create_article():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'q': \"macdonalds\",\n",
    "    'hl': 'en-US',\n",
    "    'gl': 'US',\n",
    "    'ceid': 'US:en',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://news.google.com/search', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = fromstring(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date):\n",
    "    return(date)\n",
    "\n",
    "def format_author(author):\n",
    "    return(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_base64(encoded_url):\n",
    "    # Decode the base64-encoded URL and convert it to a UTF-8 string\n",
    "    decoded_string = base64.b64decode(encoded_url).decode(\"utf-8\")\n",
    "    \n",
    "    # The line `return json.loads(decoded_string)[-1]` is decoding a JSON string and returning the\n",
    "    # last element of the resulting list.\n",
    "    return json.loads(decoded_string)[-1]\n",
    "\n",
    "\n",
    "def extract_base64_string(url):\n",
    "    # Split the URL by semicolon, then further split the second part by colon to extract the base64 string\n",
    "    return url.split(\";\")[1].split(\":\")[1]\n",
    "\n",
    "def convert_url(url):\n",
    "    return(decode_base64(extract_base64_string(url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(parser):\n",
    "    L = []\n",
    "    articles = parser.xpath('//*[@id=\"yDmH0d\"]/c-wiz/div/main/div[2]/c-wiz')[0]\n",
    "    \n",
    "    for k in range(len(articles)):\n",
    "        parser_author = parser.xpath(f'//*[@id=\"yDmH0d\"]/c-wiz/div/main/div[2]/c-wiz/c-wiz[{k}]/c-wiz/article/div[2]/div/span')\n",
    "        parser_url = parser.xpath(f'//*[@id=\"yDmH0d\"]/c-wiz/div/main/div[2]/c-wiz/c-wiz[{k}]/c-wiz/article/div[1]/div[1]/a/@jslog')\n",
    "        parser_date = parser.xpath(f'//*[@id=\"yDmH0d\"]/c-wiz/div/main/div[2]/c-wiz/c-wiz[{k}]/c-wiz/article/div[2]/time/@datetime')\n",
    "        parser_title = parser.xpath(f'//*[@id=\"yDmH0d\"]/c-wiz/div/main/div[2]/c-wiz/c-wiz{k}/c-wiz/article/div[1]/div[2]/div/a/text()')\n",
    "\n",
    "\n",
    "   \n",
    "        if len(parser_url)>0:\n",
    "            try:\n",
    "                url = convert_url(parser_url[0])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                news = newspaper.Article(url)\n",
    "                news.download()\n",
    "                news.parse()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if news.text != '':\n",
    "                text = news.text\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if news.publish_date != None:\n",
    "                date = news.publish_date\n",
    "            elif len(parser_date)>0:\n",
    "                date = format_date(parser_date[0])\n",
    "            else:\n",
    "                date = format_date(-1)\n",
    "\n",
    "            if len(news.authors)>0:\n",
    "                author = ', '.join(news.authors)\n",
    "            elif len(parser_author)>0:\n",
    "                author = format_author(parser_author[0].text)\n",
    "            else:\n",
    "                author = 'No author'\n",
    "\n",
    "            if len(parser_title)>0:\n",
    "                title = parser_title[0]\n",
    "            else:\n",
    "                title = 'No title'\n",
    "\n",
    "            L.append(clarticle(title,url,date,author,[]))\n",
    "\n",
    "    return(L)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = scrape_articles(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape un url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "url = articles[5].url\n",
    "test = newspaper.Article(url)\n",
    "test.download()\n",
    "\n",
    "test.parse()\n",
    "print(test.text == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adugast/nltk_data'\n    - 'c:\\\\Users\\\\adugast\\\\HeadMind\\\\Crédit_agricole\\\\venvCA\\\\nltk_data'\n    - 'c:\\\\Users\\\\adugast\\\\HeadMind\\\\Crédit_agricole\\\\venvCA\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\adugast\\\\HeadMind\\\\Crédit_agricole\\\\venvCA\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adugast\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\testscrapehero.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adugast/HeadMind/Cr%C3%A9dit_agricole/testscrapehero.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test\u001b[39m.\u001b[39;49mnlp()\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\newspaper\\article.py:361\u001b[0m, in \u001b[0;36mArticle.nlp\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_keywords(keyws)\n\u001b[0;32m    359\u001b[0m max_sents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mMAX_SUMMARY_SENT\n\u001b[1;32m--> 361\u001b[0m summary_sents \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39;49msummarize(title\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtitle, text\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext, max_sents\u001b[39m=\u001b[39;49mmax_sents)\n\u001b[0;32m    362\u001b[0m summary \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(summary_sents)\n\u001b[0;32m    363\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_summary(summary)\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\newspaper\\nlp.py:45\u001b[0m, in \u001b[0;36msummarize\u001b[1;34m(url, title, text, max_sents)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m     44\u001b[0m summaries \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 45\u001b[0m sentences \u001b[39m=\u001b[39m split_sentences(text)\n\u001b[0;32m     46\u001b[0m keys \u001b[39m=\u001b[39m keywords(text)\n\u001b[0;32m     47\u001b[0m titleWords \u001b[39m=\u001b[39m split_words(title)\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\newspaper\\nlp.py:157\u001b[0m, in \u001b[0;36msplit_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Split a large string into sentences\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m tokenizer \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mtokenizers/punkt/english.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    159\u001b[0m sentences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n\u001b[0;32m    160\u001b[0m sentences \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sentences \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\nltk\\data.py:823\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    821\u001b[0m fil \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(path_[:\u001b[39m-\u001b[39m\u001b[39m7\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m path_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m switch_punkt(fil)\n\u001b[0;32m    824\u001b[0m \u001b[39melif\u001b[39;00m path_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mchunkers/maxent_ne_chunker\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m switch_chunker(fil\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\nltk\\data.py:678\u001b[0m, in \u001b[0;36mswitch_punkt\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[39mReturn a pickle-free Punkt tokenizer instead of loading a pickle.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[39m['Hello!', 'How are you?']\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m PunktTokenizer \u001b[39mas\u001b[39;00m tok\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m tok(lang)\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\adugast\\HeadMind\\Crédit_agricole\\venvCA\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adugast/nltk_data'\n    - 'c:\\\\Users\\\\adugast\\\\HeadMind\\\\Crédit_agricole\\\\venvCA\\\\nltk_data'\n    - 'c:\\\\Users\\\\adugast\\\\HeadMind\\\\Crédit_agricole\\\\venvCA\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\adugast\\\\HeadMind\\\\Crédit_agricole\\\\venvCA\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adugast\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "test.nlp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvCA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
